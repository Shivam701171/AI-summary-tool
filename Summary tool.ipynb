{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35154a8c-891e-4378-be36-80bea46e9e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import re\n",
    "import warnings\n",
    "from typing import List, Dict, Union, Optional\n",
    "import textwrap\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9810a51e-74a7-48d9-8179-75c108767b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DOCUMENT SUMMARIZER - HUGGING FACE MODELS\n",
      "============================================================\n",
      "Available Summarization Models:\n",
      "--------------------------------------------------\n",
      "ü§ñ LED (led)\n",
      "   Model: allenai/led-base-16384\n",
      "   Max Length: 16384 tokens\n",
      "   Description: Great for long documents, supports up to 16,384 tokens\n",
      "\n",
      "ü§ñ BART (bart)\n",
      "   Model: facebook/bart-large\n",
      "   Max Length: 1024 tokens\n",
      "   Description: Excellent for general summarization, up to 1024 tokens\n",
      "\n",
      "ü§ñ T5 (t5)\n",
      "   Model: t5-small\n",
      "   Max Length: 512 tokens\n",
      "   Description: Versatile model, good for various text types (faster but less accurate)\n",
      "\n",
      "ü§ñ T5-Base (t5_base)\n",
      "   Model: t5-base\n",
      "   Max Length: 512 tokens\n",
      "   Description: Better quality than t5-small, still relatively fast\n",
      "\n",
      "ü§ñ Pegasus (pegasus)\n",
      "   Model: google/pegasus-cnn_dailymail\n",
      "   Max Length: 1024 tokens\n",
      "   Description: Specialized for news summarization, very high quality\n",
      "\n",
      "ü§ñ Flan-T5 (flan_t5)\n",
      "   Model: google/flan-t5-small\n",
      "   Max Length: 512 tokens\n",
      "   Description: Instruction-tuned T5, follows instructions better\n",
      "\n",
      "\n",
      "============================================================\n",
      "USAGE EXAMPLES\n",
      "============================================================\n",
      "Example 1: Basic Document Summarization\n",
      "--------------------------------------------------\n",
      "Loading BART model...\n",
      "Model ID: facebook/bart-large\n",
      "WARNING:tensorflow:From C:\\Users\\Laptop22\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BART model loaded successfully!\n",
      "üì± Using device: CPU\n",
      "üìÑ Original Text:\n",
      "Artificial intelligence (AI) is rapidly transforming industries across the\n",
      "globe, from healthcare and finance to transportation and entertainment. Machine\n",
      "learning algorithms are becoming increasingly sophisticated, enabling computers\n",
      "to perform tasks that were once thought to require human intelligence. Companies\n",
      "are investing billions of dollars in AI research and development, hoping to gain\n",
      "a competitive edge in the digital economy. However, the rapid advancement of AI\n",
      "also raises important questions about job displacement, privacy, and ethical\n",
      "considerations. Experts warn that society must carefully consider the\n",
      "implications of widespread AI adoption to ensure that the benefits are shared\n",
      "equitably and that potential risks are properly managed. The future of AI will\n",
      "likely depend on how well we can balance innovation with responsibility,\n",
      "creating systems that enhance human capabilities rather than replace them\n",
      "entirely.\n",
      "\n",
      "üìä Original length: 131 words\n",
      "\n",
      "üìù Summary:\n",
      "The Future of Artificial Intelligence √Ç    Artificial intelligence (AI) is\n",
      "rapidly transforming industries across the globe, from healthcare and finance to\n",
      "transportation and entertainment. Machine learning algorithms are becoming\n",
      "increasingly sophisticated, enabling computers to perform tasks that were once\n",
      "thought to require human intelligence. Companies are investing billions of\n",
      "dollars in AI research and development, hoping to gain a competitive edge in the\n",
      "digital economy\n",
      "üìä Summary length: 65 words\n",
      "\n",
      "============================================================\n",
      "Example 2: Long Document Summarization\n",
      "--------------------------------------------------\n",
      "üìÑ Long document length: 404 words\n",
      "üìÑ Document split into 1 chunks\n",
      "Summarizing chunk 1/1...\n",
      "\n",
      "üìù Long Document Summary:\n",
      "Artificial intelligence (AI) is rapidly transforming industries across the\n",
      "globe, from healthcare and finance to transportation and entertainment.\n",
      "Companies are investing billions of dollars in AI research and development,\n",
      "hoping to gain a competitive edge in the digital economy. Machine learning\n",
      "algorithms are becoming increasingly sophisticated, enabling computers to\n",
      "perform tasks that were once thought to require human intelligence. However, the\n",
      "rapid advancement of AI also raises important questions about job displacement,\n",
      "privacy, and ethical considerations. Experts warn that society must carefully\n",
      "consider the implications of widespread AI adoption to ensure that the benefits\n",
      "are shared equitably and that potential risks are properly managed. The future\n",
      "of AI will likely depend on how well we can balance innovation with\n",
      "responsibility, creating systems that enhance human capabilities rather than\n",
      "replace them entirely. Deep learning neural networks have revolutionized the\n",
      "field of computer vision in recent years. Convolutional Neural Networks (CNNs)\n",
      "have proven particularly\n",
      "üìä Summary length: 152 words\n",
      "\n",
      "============================================================\n",
      "Example 3: Batch Summarization\n",
      "--------------------------------------------------\n",
      "üìù Summarizing document 1/3\n",
      "üìù Summarizing document 2/3\n",
      "üìù Summarizing document 3/3\n",
      "\n",
      "üìÑ News Article:\n",
      "The Future of Artificial Intelligence √Ç    Artificial intelligence (AI) is\n",
      "rapidly transforming industries across the globe, from healthcare and finance to\n",
      "transportation and entertainment. Machine learning algorithms are becoming\n",
      "increasingly sophisticated, enabling computers to perform tasks that were once\n",
      "thought to require human intelligence. Companies are investing billions of\n",
      "dollars in AI research and development, hoping to gain a competitive edge in the\n",
      "digital economy.\n",
      "\n",
      "üìÑ Technical Article:\n",
      "Deep Learning    Deep learning neural networks have revolutionized the field of\n",
      "computer vision in recent years. Convolutional Neural Networks (CNNs) have\n",
      "proven particularly effective for image recognition tasks, achieving superhuman\n",
      "performance on various benchmarks. The architecture of CNNs is inspired by the\n",
      "visual cortex of animals, using layers of convolution and pooling operations to\n",
      "extract hierarchical features from images. The\n",
      "\n",
      "üìÑ Business Report:\n",
      "E-commerce:    The global e-commerce market experienced unprecedented growth\n",
      "during 2020-2023, driven primarily by the COVID-19 pandemic and changing\n",
      "consumer behavior. Online retail sales increased by over 200% in many\n",
      "categories, with companies like Amazon, Shopify, and Alibaba reporting record\n",
      "profits. Mobile commerce (m-commerce) emerged as a dominant trend, with\n",
      "\n",
      "============================================================\n",
      "Example 4: Model Comparison\n",
      "--------------------------------------------------\n",
      "ü§ñ Testing BART model...\n",
      "Loading BART model...\n",
      "Model ID: facebook/bart-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BART model loaded successfully!\n",
      "üì± Using device: CPU\n",
      "ü§ñ Testing T5 model...\n",
      "Loading T5 model...\n",
      "Model ID: t5-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ T5 model loaded successfully!\n",
      "üì± Using device: CPU\n",
      "\n",
      "üìÑ Original Text (first 200 chars):\n",
      "\n",
      "    Deep learning neural networks have revolutionized the field of computer vision in recent years. Convolutional Neural Networks (CNNs) have proven particularly effective for image recognition tasks...\n",
      "\n",
      "ü§ñ BART Summary:\n",
      "Deep Learning    Deep learning neural networks have revolutionized the field of\n",
      "computer vision in recent years. Convolutional Neural Networks (CNNs) have\n",
      "proven particularly effective for image recognition tasks, achieving superhuman\n",
      "performance on various benchmarks. The architecture of CNNs is inspired by the\n",
      "visual cortex of animals, using layers of convolution and pooling operations to\n",
      "extract hierarchical features from images. The field continues to evolve\n",
      "rapidly, with new architectures and training techniques being developed\n",
      "regularly. Recent advances include attention\n",
      "\n",
      "ü§ñ T5 Summary:\n",
      "the architecture of CNNs is inspired by the visual cortex of animals . transfer\n",
      "learning is a popular technique, allowing practitioners to leverage pre-trained\n",
      "models and fine-tune them for specific tasks .\n",
      "\n",
      "============================================================\n",
      "Example 5: Advanced Summarization Features\n",
      "--------------------------------------------------\n",
      "Loading LED model...\n",
      "Model ID: allenai/led-base-16384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0a69e4156a444ea6a66a9cd97b8e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/648M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07fd03791b0a4f4e8a5858c853b55629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65510e83c59847b1973f7ee705141f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/27.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6378e91a898f434a8c1c2618aea47a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ecf9da8630f46fe9157082ad9e2febd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/648M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8741c5c3c5324a1f8a155464d847df6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086514410a6043f3bf7fbe59986e9620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Input ids are automatically padded from 184 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LED model loaded successfully!\n",
      "üì± Using device: CPU\n",
      "üìä Document Analysis:\n",
      "üìù Summary: \n",
      "üîë Keywords: online, commerce, consumer, over, social, shopping, experiences, global, market, experienced\n",
      "üìä Word Count: 131\n",
      "üìä Sentences: 8\n",
      "‚è±Ô∏è Reading Time: 1 minutes\n",
      "\n",
      "============================================================\n",
      "Example 6: Custom Document Processor\n",
      "--------------------------------------------------\n",
      "üìÑ Processing Custom Document:\n",
      "Loading BART model...\n",
      "Model ID: facebook/bart-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BART model loaded successfully!\n",
      "üì± Using device: CPU\n",
      "Source: Direct input\n",
      "Original Length: 158 words\n",
      "Summary: Machine learning is a subset of artificial intelligence that focuses on the development of algorithms and statistical models that enable computer systems to improve their performance on a specific task through experience, without being explicitly programmed. The core idea behind machine learning is to create systems that can automatically learn and improve from data, identifying patterns and making predictions or decisions based on input information. As data becomes increasingly abundant and computational power continues to grow, machine learning techniques are becoming more sophisticated and widely adopted across industries. Machine learning has applications across numerous fields including computer vision, natural language processing, recommendation systems, fraud detection, medical diagnosis, and autonomous vehicles. There are three main types of machine learning: supervised learning, where algorithms learn from labeled training data; unsupervised learning,\n",
      "Summary Length: 129 words\n",
      "Compression Ratio: 81.6%\n",
      "\n",
      "============================================================\n",
      "INSTALLATION AND SETUP INSTRUCTIONS\n",
      "============================================================\n",
      "\n",
      "üì¶ REQUIRED PACKAGES:\n",
      "   pip install transformers torch torchvision torchaudio\n",
      "   pip install accelerate  # For faster loading\n",
      "   pip install sentencepiece  # For T5 models\n",
      "\n",
      "üöÄ QUICK START:\n",
      "   1. Copy this code to your Python file/Jupyter notebook\n",
      "   2. Install required packages\n",
      "   3. Run the examples above\n",
      "   4. Replace sample text with your own documents\n",
      "\n",
      "üí° TIPS FOR BETTER PERFORMANCE:\n",
      "   ‚Ä¢ Use GPU if available (automatically detected)\n",
      "   ‚Ä¢ Start with smaller models (t5-small) for testing\n",
      "   ‚Ä¢ Use BART for general text, Pegasus for news\n",
      "   ‚Ä¢ Chunk long documents for better quality\n",
      "   ‚Ä¢ Experiment with max_length and min_length\n",
      "\n",
      "üéØ RECOMMENDED MODELS BY USE CASE:\n",
      "   ‚Ä¢ News Articles: facebook/bart-large-cnn or google/pegasus-cnn_dailymail\n",
      "   ‚Ä¢ Academic Papers: t5-base or google/flan-t5-base  \n",
      "   ‚Ä¢ General Text: facebook/bart-large-cnn\n",
      "   ‚Ä¢ Conversations: philschmid/bart-large-cnn-samsum\n",
      "   ‚Ä¢ Fast Processing: t5-small (lower quality but faster)\n",
      "\n",
      "‚ö° PERFORMANCE NOTES:\n",
      "   ‚Ä¢ First run downloads models (~500MB-2GB)\n",
      "   ‚Ä¢ GPU significantly speeds up processing\n",
      "   ‚Ä¢ Models are cached after first download\n",
      "   ‚Ä¢ Batch processing is more efficient for multiple documents\n",
      "\n",
      "\n",
      "‚úÖ Ready to use! Try replacing the sample documents with your own text.\n",
      "üéØ Start with: summarizer = DocumentSummarizer('bart')\n",
      "üìù Then use: summary = summarizer.summarize('Your text here')\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DOCUMENT SUMMARIZER USING HUGGING FACE PRE-TRAINED MODELS\n",
    "# Complete toolkit for text summarization\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DOCUMENT SUMMARIZER - HUGGING FACE MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL CONFIGURATIONS\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for different summarization models\"\"\"\n",
    "    name: str\n",
    "    model_id: str\n",
    "    max_input_length: int\n",
    "    description: str\n",
    "\n",
    "# Available pre-trained models\n",
    "AVAILABLE_MODELS = {\n",
    "    \"led\": ModelConfig(\n",
    "        name=\"LED\",\n",
    "        model_id=\"allenai/led-base-16384\",\n",
    "        max_input_length=16384,\n",
    "        description=\"Great for long documents, supports up to 16,384 tokens\"\n",
    "    ),\n",
    "    \"bart\": ModelConfig(\n",
    "        name=\"BART\",\n",
    "        model_id=\"facebook/bart-large\",\n",
    "        max_input_length=1024,\n",
    "        description=\"Excellent for general summarization, up to 1024 tokens\"\n",
    "    ),\n",
    "    \"t5\": ModelConfig(\n",
    "        name=\"T5\",\n",
    "        model_id=\"t5-small\",\n",
    "        max_input_length=512,\n",
    "        description=\"Versatile model, good for various text types (faster but less accurate)\"\n",
    "    ),\n",
    "    \"t5_base\": ModelConfig(\n",
    "        name=\"T5-Base\",\n",
    "        model_id=\"t5-base\",\n",
    "        max_input_length=512,\n",
    "        description=\"Better quality than t5-small, still relatively fast\"\n",
    "    ),\n",
    "    \"pegasus\": ModelConfig(\n",
    "        name=\"Pegasus\",\n",
    "        model_id=\"google/pegasus-cnn_dailymail\",\n",
    "        max_input_length=1024,\n",
    "        description=\"Specialized for news summarization, very high quality\"\n",
    "    ),\n",
    "    \"flan_t5\": ModelConfig(\n",
    "        name=\"Flan-T5\",\n",
    "        model_id=\"google/flan-t5-small\",\n",
    "        max_input_length=512,\n",
    "        description=\"Instruction-tuned T5, follows instructions better\"\n",
    "    )\n",
    "}\n",
    "\n",
    "def list_available_models():\n",
    "    \"\"\"Display available models\"\"\"\n",
    "    print(\"Available Summarization Models:\")\n",
    "    print(\"-\" * 50)\n",
    "    for key, config in AVAILABLE_MODELS.items():\n",
    "        print(f\"ü§ñ {config.name} ({key})\")\n",
    "        print(f\"   Model: {config.model_id}\")\n",
    "        print(f\"   Max Length: {config.max_input_length} tokens\")\n",
    "        print(f\"   Description: {config.description}\")\n",
    "        print()\n",
    "\n",
    "list_available_models()\n",
    "\n",
    "# =============================================================================\n",
    "# SIMPLE SUMMARIZER CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class DocumentSummarizer:\n",
    "    \"\"\"\n",
    "    Simple document summarizer using Hugging Face models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"bart\", device: str = \"auto\"):\n",
    "        \"\"\"\n",
    "        Initialize the summarizer\n",
    "        \n",
    "        Args:\n",
    "            model_name: Model to use ('bart', 't5', 'pegasus', etc.)\n",
    "            device: Device to use ('auto', 'cpu', 'cuda')\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.config = AVAILABLE_MODELS.get(model_name, AVAILABLE_MODELS[\"led\"])\n",
    "        \n",
    "        print(f\"Loading {self.config.name} model...\")\n",
    "        print(f\"Model ID: {self.config.model_id}\")\n",
    "        \n",
    "        # Initialize the summarization pipeline\n",
    "        self.summarizer = pipeline(\n",
    "            \"summarization\",\n",
    "            model=self.config.model_id,\n",
    "            tokenizer=self.config.model_id,\n",
    "            device=0 if device == \"auto\" and torch.cuda.is_available() else -1\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ {self.config.name} model loaded successfully!\")\n",
    "        print(f\"üì± Using device: {'GPU' if self.summarizer.device.type == 'cuda' else 'CPU'}\")\n",
    "    \n",
    "    def summarize(self, \n",
    "                  text: str, \n",
    "                  max_length: int = 150, \n",
    "                  min_length: int = 50,\n",
    "                  do_sample: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Summarize a single document\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to summarize\n",
    "            max_length: Maximum length of summary\n",
    "            min_length: Minimum length of summary\n",
    "            do_sample: Whether to use sampling (more creative but less consistent)\n",
    "            \n",
    "        Returns:\n",
    "            Generated summary\n",
    "        \"\"\"\n",
    "        # Handle special case for T5 models (need \"summarize:\" prefix)\n",
    "        if \"t5\" in self.config.model_id.lower():\n",
    "            text = f\"summarize: {text}\"\n",
    "        \n",
    "        try:\n",
    "            # Generate summary\n",
    "            summary = self.summarizer(\n",
    "                text,\n",
    "                max_length=max_length,\n",
    "                min_length=min_length,\n",
    "                do_sample=do_sample,\n",
    "                truncation=True\n",
    "            )\n",
    "            \n",
    "            return summary[0]['summary_text']\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating summary: {str(e)}\"\n",
    "    \n",
    "    def summarize_long_document(self, \n",
    "                               text: str, \n",
    "                               chunk_size: int = None,\n",
    "                               max_length: int = 150,\n",
    "                               min_length: int = 50) -> str:\n",
    "        \"\"\"\n",
    "        Summarize long documents by chunking\n",
    "        \n",
    "        Args:\n",
    "            text: Long input text\n",
    "            chunk_size: Size of each chunk (auto-calculated if None)\n",
    "            max_length: Maximum length of each chunk summary\n",
    "            min_length: Minimum length of each chunk summary\n",
    "            \n",
    "        Returns:\n",
    "            Combined summary\n",
    "        \"\"\"\n",
    "        if chunk_size is None:\n",
    "            chunk_size = self.config.max_input_length - 100  # Leave some buffer\n",
    "        \n",
    "        # Split text into chunks\n",
    "        chunks = self._split_text_into_chunks(text, chunk_size)\n",
    "        \n",
    "        print(f\"üìÑ Document split into {len(chunks)} chunks\")\n",
    "        \n",
    "        # Summarize each chunk\n",
    "        chunk_summaries = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"Summarizing chunk {i+1}/{len(chunks)}...\")\n",
    "            summary = self.summarize(chunk, max_length, min_length)\n",
    "            chunk_summaries.append(summary)\n",
    "        \n",
    "        # Combine summaries\n",
    "        combined_summary = \" \".join(chunk_summaries)\n",
    "        \n",
    "        # If combined summary is still too long, summarize it again\n",
    "        if len(combined_summary.split()) > max_length:\n",
    "            print(\"üîÑ Combined summary too long, creating final summary...\")\n",
    "            final_summary = self.summarize(combined_summary, max_length, min_length)\n",
    "            return final_summary\n",
    "        \n",
    "        return combined_summary\n",
    "    \n",
    "    def _split_text_into_chunks(self, text: str, chunk_size: int) -> List[str]:\n",
    "        \"\"\"Split text into chunks of approximately chunk_size tokens\"\"\"\n",
    "        # Simple word-based chunking (rough approximation)\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = \" \".join(words[i:i + chunk_size])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def batch_summarize(self, \n",
    "                       texts: List[str], \n",
    "                       max_length: int = 150,\n",
    "                       min_length: int = 50) -> List[str]:\n",
    "        \"\"\"\n",
    "        Summarize multiple documents\n",
    "        \n",
    "        Args:\n",
    "            texts: List of input texts\n",
    "            max_length: Maximum length of summaries\n",
    "            min_length: Minimum length of summaries\n",
    "            \n",
    "        Returns:\n",
    "            List of summaries\n",
    "        \"\"\"\n",
    "        summaries = []\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            print(f\"üìù Summarizing document {i+1}/{len(texts)}\")\n",
    "            summary = self.summarize(text, max_length, min_length)\n",
    "            summaries.append(summary)\n",
    "        \n",
    "        return summaries\n",
    "\n",
    "# =============================================================================\n",
    "# USAGE EXAMPLES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"USAGE EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sample documents for testing\n",
    "sample_documents = {\n",
    "    \"news_article\": \"\"\"\n",
    "    Artificial intelligence (AI) is rapidly transforming industries across the globe, from healthcare and finance to transportation and entertainment. Machine learning algorithms are becoming increasingly sophisticated, enabling computers to perform tasks that were once thought to require human intelligence. Companies are investing billions of dollars in AI research and development, hoping to gain a competitive edge in the digital economy. However, the rapid advancement of AI also raises important questions about job displacement, privacy, and ethical considerations. Experts warn that society must carefully consider the implications of widespread AI adoption to ensure that the benefits are shared equitably and that potential risks are properly managed. The future of AI will likely depend on how well we can balance innovation with responsibility, creating systems that enhance human capabilities rather than replace them entirely.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"technical_article\": \"\"\"\n",
    "    Deep learning neural networks have revolutionized the field of computer vision in recent years. Convolutional Neural Networks (CNNs) have proven particularly effective for image recognition tasks, achieving superhuman performance on various benchmarks. The architecture of CNNs is inspired by the visual cortex of animals, using layers of convolution and pooling operations to extract hierarchical features from images. Transfer learning has become a popular technique, allowing practitioners to leverage pre-trained models and fine-tune them for specific tasks with limited data. Popular frameworks like TensorFlow and PyTorch have made deep learning more accessible to researchers and developers. Recent advances include attention mechanisms, which help models focus on relevant parts of the input, and transformer architectures, which have shown remarkable success in both natural language processing and computer vision tasks. The field continues to evolve rapidly, with new architectures and training techniques being developed regularly.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"business_report\": \"\"\"\n",
    "    The global e-commerce market experienced unprecedented growth during 2020-2023, driven primarily by the COVID-19 pandemic and changing consumer behavior. Online retail sales increased by over 200% in many categories, with companies like Amazon, Shopify, and Alibaba reporting record profits. Small businesses rapidly adopted digital transformation strategies, moving their operations online to survive lockdowns and social distancing measures. Mobile commerce (m-commerce) emerged as a dominant trend, with over 70% of online purchases now made through smartphones and tablets. Social media platforms integrated shopping features, creating new opportunities for direct-to-consumer brands. Supply chain disruptions highlighted the importance of diversified logistics networks and local fulfillment centers. Looking ahead, experts predict that hybrid shopping experiences combining online and offline elements will become the new standard, as consumers increasingly expect seamless omnichannel experiences across all touchpoints.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE 1: BASIC SUMMARIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Example 1: Basic Document Summarization\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Initialize summarizer (you can change the model here)\n",
    "summarizer = DocumentSummarizer(model_name=\"bart\")  \n",
    "\n",
    "# Summarize a news article\n",
    "news_text = sample_documents[\"news_article\"]\n",
    "print(\"üìÑ Original Text:\")\n",
    "print(textwrap.fill(news_text.strip(), width=80))\n",
    "print(f\"\\nüìä Original length: {len(news_text.split())} words\")\n",
    "\n",
    "summary = summarizer.summarize(\n",
    "    news_text,\n",
    "    max_length=79,\n",
    "    min_length=30\n",
    ")\n",
    "\n",
    "print(f\"\\nüìù Summary:\")\n",
    "print(textwrap.fill(summary, width=80))\n",
    "print(f\"üìä Summary length: {len(summary.split())} words\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE 2: LONG DOCUMENT SUMMARIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example 2: Long Document Summarization\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create a longer document by combining samples\n",
    "long_document = \"\\n\\n\".join(sample_documents.values())\n",
    "print(f\"üìÑ Long document length: {len(long_document.split())} words\")\n",
    "\n",
    "long_summary = summarizer.summarize_long_document(\n",
    "    long_document,\n",
    "    max_length=179,\n",
    "    min_length=80\n",
    ")\n",
    "\n",
    "print(f\"\\nüìù Long Document Summary:\")\n",
    "print(textwrap.fill(long_summary, width=80))\n",
    "print(f\"üìä Summary length: {len(long_summary.split())} words\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE 3: BATCH SUMMARIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example 3: Batch Summarization\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Summarize multiple documents\n",
    "document_list = list(sample_documents.values())\n",
    "batch_summaries = summarizer.batch_summarize(\n",
    "    document_list,\n",
    "    max_length=80,\n",
    "    min_length=30\n",
    ")\n",
    "\n",
    "for i, (doc_type, summary) in enumerate(zip(sample_documents.keys(), batch_summaries)):\n",
    "    print(f\"\\nüìÑ {doc_type.replace('_', ' ').title()}:\")\n",
    "    print(textwrap.fill(summary, width=80))\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE 4: DIFFERENT MODELS COMPARISON\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example 4: Model Comparison\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def compare_models(text: str, models: List[str] = [\"led\", \"t5\"]):\n",
    "    \"\"\"Compare summaries from different models\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for model_name in models:\n",
    "        try:\n",
    "            print(f\"ü§ñ Testing {model_name.upper()} model...\")\n",
    "            temp_summarizer = DocumentSummarizer(model_name=model_name)\n",
    "            summary = temp_summarizer.summarize(text, max_length=100, min_length=30)\n",
    "            results[model_name] = summary\n",
    "            # Clean up to save memory\n",
    "            del temp_summarizer\n",
    "        except Exception as e:\n",
    "            results[model_name] = f\"Error: {str(e)}\"\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare models on the same text\n",
    "comparison_text = sample_documents[\"technical_article\"]\n",
    "model_comparisons = compare_models(comparison_text, [\"bart\", \"t5\"])\n",
    "\n",
    "print(f\"\\nüìÑ Original Text (first 200 chars):\")\n",
    "print(comparison_text[:200] + \"...\")\n",
    "\n",
    "for model, summary in model_comparisons.items():\n",
    "    print(f\"\\nü§ñ {model.upper()} Summary:\")\n",
    "    print(textwrap.fill(summary, width=80))\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE 5: ADVANCED FEATURES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example 5: Advanced Summarization Features\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "class AdvancedSummarizer(DocumentSummarizer):\n",
    "    \"\"\"Advanced summarizer with additional features\"\"\"\n",
    "    \n",
    "    def extractive_keywords(self, text: str, top_k: int = 10) -> List[str]:\n",
    "        \"\"\"Extract keywords from text (simple frequency-based)\"\"\"\n",
    "        # Remove common stop words\n",
    "        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those'}\n",
    "        \n",
    "        # Simple keyword extraction\n",
    "        words = re.findall(r'\\b[a-zA-Z]{4,}\\b', text.lower())\n",
    "        word_freq = {}\n",
    "        \n",
    "        for word in words:\n",
    "            if word not in stop_words:\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "        \n",
    "        # Return top keywords\n",
    "        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [word for word, freq in sorted_words[:top_k]]\n",
    "    \n",
    "    def analyze_document(self, text: str) -> Dict[str, Union[str, List[str], int]]:\n",
    "        \"\"\"Comprehensive document analysis\"\"\"\n",
    "        return {\n",
    "            \"summary\": self.summarize(text),\n",
    "            \"keywords\": self.extractive_keywords(text),\n",
    "            \"word_count\": len(text.split()),\n",
    "            \"sentence_count\": len(re.split(r'[.!?]+', text)),\n",
    "            \"estimated_reading_time\": f\"{len(text.split()) // 200 + 1} minutes\"\n",
    "        }\n",
    "\n",
    "# Demonstrate advanced features\n",
    "advanced_summarizer = AdvancedSummarizer(model_name=\"LED\")\n",
    "analysis = advanced_summarizer.analyze_document(sample_documents[\"business_report\"])\n",
    "\n",
    "print(\"üìä Document Analysis:\")\n",
    "print(f\"üìù Summary: {textwrap.fill(analysis['summary'], width=70)}\")\n",
    "print(f\"üîë Keywords: {', '.join(analysis['keywords'])}\")\n",
    "print(f\"üìä Word Count: {analysis['word_count']}\")\n",
    "print(f\"üìä Sentences: {analysis['sentence_count']}\")\n",
    "print(f\"‚è±Ô∏è Reading Time: {analysis['estimated_reading_time']}\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE 6: CUSTOM DOCUMENT PROCESSOR\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example 6: Custom Document Processor\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def process_your_document(file_path_or_text: str, \n",
    "                         model_name: str = \"bart\",\n",
    "                         summary_length: str = \"medium\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Process your own document\n",
    "    \n",
    "    Args:\n",
    "        file_path_or_text: Path to text file or direct text input\n",
    "        model_name: Model to use for summarization\n",
    "        summary_length: \"short\", \"medium\", or \"long\"\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with summary and metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    # Length configurations\n",
    "    length_configs = {\n",
    "        \"short\": {\"max_length\": 80, \"min_length\": 20},\n",
    "        \"medium\": {\"max_length\": 150, \"min_length\": 50},\n",
    "        \"long\": {\"max_length\": 300, \"min_length\": 100}\n",
    "    }\n",
    "    \n",
    "    config = length_configs.get(summary_length, length_configs[\"medium\"])\n",
    "    \n",
    "    # Determine if input is file path or text\n",
    "    if len(file_path_or_text) < 1000 and (file_path_or_text.endswith('.txt') or '/' in file_path_or_text or '\\\\' in file_path_or_text):\n",
    "        try:\n",
    "            with open(file_path_or_text, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "            source = f\"File: {file_path_or_text}\"\n",
    "        except:\n",
    "            text = file_path_or_text\n",
    "            source = \"Direct input\"\n",
    "    else:\n",
    "        text = file_path_or_text\n",
    "        source = \"Direct input\"\n",
    "    \n",
    "    # Initialize summarizer\n",
    "    summarizer = DocumentSummarizer(model_name=model_name)\n",
    "    \n",
    "    # Choose summarization method based on text length\n",
    "    if len(text.split()) > 1000:\n",
    "        summary = summarizer.summarize_long_document(text, **config)\n",
    "    else:\n",
    "        summary = summarizer.summarize(text, **config)\n",
    "    \n",
    "    return {\n",
    "        \"source\": source,\n",
    "        \"original_length\": f\"{len(text.split())} words\",\n",
    "        \"summary\": summary,\n",
    "        \"summary_length\": f\"{len(summary.split())} words\",\n",
    "        \"compression_ratio\": f\"{len(summary.split()) / len(text.split()) * 100:.1f}%\"\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "print(\"üìÑ Processing Custom Document:\")\n",
    "\n",
    "# You can replace this with your own text or file path\n",
    "your_text = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence that focuses on the development of algorithms and statistical models that enable computer systems to improve their performance on a specific task through experience, without being explicitly programmed. The core idea behind machine learning is to create systems that can automatically learn and improve from data, identifying patterns and making predictions or decisions based on input information. There are three main types of machine learning: supervised learning, where algorithms learn from labeled training data; unsupervised learning, where systems find hidden patterns in data without labeled examples; and reinforcement learning, where agents learn through interaction with an environment by receiving rewards or penalties for their actions. Machine learning has applications across numerous fields including computer vision, natural language processing, recommendation systems, fraud detection, medical diagnosis, and autonomous vehicles. As data becomes increasingly abundant and computational power continues to grow, machine learning techniques are becoming more sophisticated and widely adopted across industries.\n",
    "\"\"\"\n",
    "\n",
    "result = process_your_document(\n",
    "    your_text,\n",
    "    model_name=\"bart\",\n",
    "    summary_length=\"medium\"\n",
    ")\n",
    "\n",
    "for key, value in result.items():\n",
    "    print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "# =============================================================================\n",
    "# INSTALLATION AND SETUP INSTRUCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INSTALLATION AND SETUP INSTRUCTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "setup_instructions = \"\"\"\n",
    "üì¶ REQUIRED PACKAGES:\n",
    "   pip install transformers torch torchvision torchaudio\n",
    "   pip install accelerate  # For faster loading\n",
    "   pip install sentencepiece  # For T5 models\n",
    "\n",
    "üöÄ QUICK START:\n",
    "   1. Copy this code to your Python file/Jupyter notebook\n",
    "   2. Install required packages\n",
    "   3. Run the examples above\n",
    "   4. Replace sample text with your own documents\n",
    "\n",
    "üí° TIPS FOR BETTER PERFORMANCE:\n",
    "   ‚Ä¢ Use GPU if available (automatically detected)\n",
    "   ‚Ä¢ Start with smaller models (t5-small) for testing\n",
    "   ‚Ä¢ Use BART for general text, Pegasus for news\n",
    "   ‚Ä¢ Chunk long documents for better quality\n",
    "   ‚Ä¢ Experiment with max_length and min_length\n",
    "\n",
    "üéØ RECOMMENDED MODELS BY USE CASE:\n",
    "   ‚Ä¢ News Articles: facebook/bart-large-cnn or google/pegasus-cnn_dailymail\n",
    "   ‚Ä¢ Academic Papers: t5-base or google/flan-t5-base  \n",
    "   ‚Ä¢ General Text: facebook/bart-large-cnn\n",
    "   ‚Ä¢ Conversations: philschmid/bart-large-cnn-samsum\n",
    "   ‚Ä¢ Fast Processing: t5-small (lower quality but faster)\n",
    "\n",
    "‚ö° PERFORMANCE NOTES:\n",
    "   ‚Ä¢ First run downloads models (~500MB-2GB)\n",
    "   ‚Ä¢ GPU significantly speeds up processing\n",
    "   ‚Ä¢ Models are cached after first download\n",
    "   ‚Ä¢ Batch processing is more efficient for multiple documents\n",
    "\"\"\"\n",
    "\n",
    "print(setup_instructions)\n",
    "\n",
    "print(\"\\n‚úÖ Ready to use! Try replacing the sample documents with your own text.\")\n",
    "print(\"üéØ Start with: summarizer = DocumentSummarizer('bart')\")\n",
    "print(\"üìù Then use: summary = summarizer.summarize('Your text here')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef2a80f9-e6e6-4a1d-9583-c9031365dc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BART model...\n",
      "Model ID: facebook/bart-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BART model loaded successfully!\n",
      "üì± Using device: CPU\n",
      "Source: Direct input\n",
      "Original Length: 399 words\n",
      "Summary: How Non-Linearity Works in Neural Networks (Neural Networks) (NBNs) is a way for neural networks to \"bend\" or transform the decision boundary or function to fit complex patterns. This bending helps the model cover or classify data points that don‚Äôt follow a straight-line relationship, such as curves, thresholds, or irregular shapes. This is what a linear activation function (<math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi><mo>=</mo><min>2</mn><mi><x</mi>mo>+</mo></mn>3</mn></m\n",
      "Summary Length: 60 words\n",
      "Compression Ratio: 15.0%\n"
     ]
    }
   ],
   "source": [
    "your_text = \"\"\"\n",
    "Non-Linearity as \"Bending the Line\"\n",
    "\n",
    "Straight Line (Linear): A straight line on a graph represents a linear relationship, where the output changes proportionally with the input (e.g., <math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi><mo>=</mo><mn>2</mn><mi>x</mi><mo>+</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\"> y = 2x + 3 </annotation></semantics></math>). This is what a linear activation function (<math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>x</mi></mrow><annotation encoding=\"application/x-tex\"> f(x) = x </annotation></semantics></math>) produces. However, real-world data points often don‚Äôt fall neatly along a straight line‚Äîthink of patterns like circles, curves, or scattered clusters.\n",
    "Bending the Line (Non-Linear): Non-linear activation functions (like ReLU, sigmoid, or tanh) allow the model to \"bend\" or transform the decision boundary or function to fit complex patterns. This bending helps the model cover or classify data points that don‚Äôt follow a straight-line relationship, such as curves, thresholds, or irregular shapes.\n",
    "\n",
    "Why Bending is Needed\n",
    "Imagine you‚Äôre plotting data points on a graph and trying to separate or predict them:\n",
    "\n",
    "If the points form a circular pattern (e.g., points inside vs. outside a circle), a straight line can‚Äôt separate them effectively.\n",
    "A non-linear function, enabled by activation functions like ReLU, allows the model to create a curved or piecewise boundary that can \"wrap around\" or better fit those points.\n",
    "\n",
    "This bending is what allows neural networks to solve complex problems like image recognition or speech processing, where relationships between inputs (e.g., pixels, audio signals) and outputs (e.g., \"cat\" or \"dog\") are highly non-linear.\n",
    "How Non-Linearity Works in Neural Networks\n",
    "\n",
    "Each neuron in a neural network computes a linear transformation (<math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>z</mi><mo>=</mo><mi>W</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\"> z = Wx + b </annotation></semantics></math>), which, if left alone, would produce a straight-line relationship.\n",
    "A non-linear activation function (e.g., ReLU: <math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>max</mi><mo>‚Å°</mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\"> f(x) = \\max(0, x) </annotation></semantics></math>) is applied to this output, introducing a \"bend\" or change in behavior (e.g., ReLU sets negative values to 0, creating a kink).\n",
    "When multiple layers with non-linear activations are stacked, these bends combine to form complex curves or surfaces that can fit intricate data patterns.\n",
    "\n",
    "Visualizing the Idea\n",
    "To illustrate your analogy of bending the line, let‚Äôs consider a simple example. Suppose you have data points that form a parabolic pattern (e.g., <math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi><mo>=</mo><msup><mi>x</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\"> y = x^2 </annotation></semantics></math>) rather than a straight line. Below is a chart comparing:\n",
    "\n",
    "A linear model (straight line) that fails to fit the data well.\n",
    "A non-linear model (enabled by something like ReLU in a neural network) that bends to fit the parabolic pattern.\n",
    "\"\"\"\n",
    "\n",
    "result = process_your_document(\n",
    "    your_text,\n",
    "    model_name=\"bart\",\n",
    "    summary_length=\"medium\"\n",
    ")\n",
    "\n",
    "for key, value in result.items():\n",
    "    print(f\"{key.replace('_', ' ').title()}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d7e878-cfde-4be5-b542-f76b53078f75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
